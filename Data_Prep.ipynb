{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc \n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.24.2'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('64bit', 'WindowsPE')\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "print(platform.architecture())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ep17\\Documents\\txt\n"
     ]
    }
   ],
   "source": [
    "%cd C:\\Users\\ep17\\Documents\\txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready to start\n"
     ]
    }
   ],
   "source": [
    "print('Ready to start')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_convert(sec):\n",
    "    mins = sec // 60\n",
    "    sec = sec % 60\n",
    "    hours = mins // 60\n",
    "    mins = mins % 60\n",
    "    return (\"Time Lapsed = {}:{}:{}\".format(int(hours), int(mins), int(sec)))\n",
    "\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edit_data(df, cols):\n",
    "    print('Editing...')\n",
    "    \n",
    "    # Convert categorical variables 'missing' to NaN\n",
    "    turn_to_cat = ['mba_delinquency_status', #  X = Unknown/Missing LPI date\n",
    "    'orig_active_status',\n",
    "    'current_investor_code', # U = No Info\n",
    "    'current_product_type', # NULL = No Info\n",
    "    'mba_worst_ever', # U = No Info\n",
    "    'property_type', # U = No Info\n",
    "    'occupancy_type', # U = No Info\n",
    "    'product_type', # U = No Info\n",
    "    'loan_type', # U = No Info\n",
    "    'loan_purpose', # U = No Info\n",
    "    'payment_frequency', # U = No Info\n",
    "    'channel', # U = No Info\n",
    "    'documentation_type',  # U = No Info\n",
    "    'gse_eligible_flag',  # U = No Info\n",
    "    'payment_frequency'] # U = No Info\n",
    "    \n",
    "    c_cat = []\n",
    "    for i in cols:\n",
    "        if i in turn_to_cat:\n",
    "            c_cat.append(i)\n",
    "    \n",
    "    for i in c_cat:\n",
    "        print(i)\n",
    "        if True in df[i].astype(str).isin(['U']).values.tolist():\n",
    "            df.loc[(df[i] == 'U'), i] = np.NaN\n",
    "        elif True in df[i].astype(str).isin(['X']).values.tolist():\n",
    "            df.loc[(df[i] == 'X'), i] = np.NaN\n",
    "        elif True in df[i].astype(str).isin(['NULL']).values.tolist():\n",
    "            df.loc[(df[i] == 'NULL'), i] = np.NaN\n",
    "            \n",
    "    # Convert binary variables 'missing' to NaN\n",
    "    turn_to_dummies = ['prepay_penalty_flag',\n",
    "    'collateral_type', \n",
    "    'product_type_category', \n",
    "    'loan_purpose_category', \n",
    "    'mortgage_insurance_flag', \n",
    "    'active_status', # no NA\n",
    "    'bk_flag', # no NA\n",
    "    'buydown_flag', \n",
    "    'convertible_flag', # no NA\n",
    "    'pool_insurance_flag', \n",
    "    'negative_amortization_flag', \n",
    "    'io_flag', \n",
    "    'paid_off_flag', \n",
    "    'inferred_collateral_type']\n",
    "\n",
    "    c_dum = []\n",
    "    for i in cols:\n",
    "        if i in turn_to_dummies:\n",
    "            c_dum.append(i)\n",
    "    \n",
    "    for i in c_dum:\n",
    "        print(i)\n",
    "        unique_values = df[i].astype(str).unique().tolist()\n",
    "        if True in df[i].astype(str).isin(['U']).values.tolist():\n",
    "            unique_values.remove('U')\n",
    "            df.loc[(df[i] == 'U'), i] = np.NaN\n",
    "        \n",
    "        if len(unique_values) > 2:\n",
    "            print(\"ERROR!!!!!! more than 2 unique values\")\n",
    "            print(unique_values, '| Removing...', unique_values[2])\n",
    "            df.loc[(df[i] == unique_values[2]).values, i] = np.NaN        \n",
    "    \n",
    "    print('Done editing. {} \\n'.format(time_convert(time.time() - start_time)))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_loan_counts(df):\n",
    "    df['one'] = 1\n",
    "    counts_df = df.groupby(by=['loan_id'])[['one']].sum().reset_index()\n",
    "    counts_df.columns = ['loan_id', 'counts']\n",
    "    df = df.merge(counts_df, on='loan_id', how='inner', validate='many_to_one')\n",
    "    df = df.drop(columns=['one'])\n",
    "    print('Done merging \\n')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time code execution\n",
    "def time_convert(sec):\n",
    "    mins = sec // 60\n",
    "    sec = sec % 60\n",
    "    hours = mins // 60\n",
    "    mins = mins % 60\n",
    "    return (\"Time Lapsed = {}:{}:{}\".format(int(hours), int(mins), int(sec)))\n",
    "    \n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get range of values of each column in specified dataframe\n",
    "def get_ranges(df):\n",
    "    neg_numbers = []\n",
    "    df = df.select_dtypes(include=np.number)\n",
    "    for col in df.columns:\n",
    "        print('{} range of values: [{}, {}]'.format(col, df[col].min(), df[col].max()))\n",
    "        if df[col].min() < 0:\n",
    "            neg_numbers.append(col)\n",
    "    return neg_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with strange values in any column\n",
    "def remove_strange(df, neg_cols):\n",
    "    for col in neg_cols:\n",
    "        print(col)\n",
    "        display(df[df[col] < 0])\n",
    "        df = df[df[col] > 0]\n",
    "\n",
    "    display(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get name of dataframe (for printing purposes)\n",
    "def get_df_name(df):\n",
    "    if df is all_performances:\n",
    "        name = 'Performances Dataset'\n",
    "    elif df is all_origination:\n",
    "        name = 'Origination Dataset'\n",
    "    else:\n",
    "        name = ''\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of null values in each column and each row\n",
    "def count_null(df, thresh):\n",
    "    print('***{}***'.format(get_df_name(df)))\n",
    "    print('Columns = {}. Rows = {}.'.format(df.shape[1], df.shape[0]))\n",
    "    t = str(int(thresh*100))\n",
    "\n",
    "    null_rows = df.isnull().sum(axis=1).values\n",
    "    fiftyp_rows = null_rows/len(df.columns)\n",
    "    fiftyp_rows = len(fiftyp_rows[fiftyp_rows>thresh])\n",
    "    print('There are no rows with more than {} NaN columns. {} rows have more than {}% NaN columns.'.format(null_rows.max(), fiftyp_rows, t))\n",
    "\n",
    "    null_cols = df.isnull().sum(axis=0).values\n",
    "    fiftyp_cols = null_cols/len(df)\n",
    "    fiftyp_cols = len(fiftyp_cols[fiftyp_cols>thresh])\n",
    "    print('There are no columns with more than {} NaN rows. {} columns have more than {}% NaN rows. \\n'.format(null_cols.max(), fiftyp_cols, t))\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove columns with > x NaN values in the rows\n",
    "def drop_columns(df, thresh):\n",
    "    print('***{}***'.format(get_df_name(df)))\n",
    "    print('Shape before: {}'.format(df.shape))\n",
    "    cutoff = len(df) * thresh\n",
    "    df = df.drop(columns = (df.columns[df.isna().sum().values > cutoff]))\n",
    "    print('Shape after: {} \\n'.format(df.shape))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_performance_data(year):\n",
    "    \n",
    "    cols = ['loan_id', 'current_balance', 'current_interest_rate', \n",
    "             'total_payment_due', 'scheduled_principal', 'scheduled_monthly_pi', \n",
    "             'mba_delinquency_status', 'mba_days_delinquent', 'active_status', \n",
    "             'period_of_payment', 'current_investor_code', 'current_product_type', \n",
    "             'loan_age', 'mba_worst_ever', 'bk_flag']\n",
    "    \n",
    "    i=1\n",
    "    for chunk in pd.read_csv(('Performance_Firsts_' + year + '.txt'), sep='|', usecols = cols, low_memory=False, chunksize=50000000):\n",
    "        performance_data = chunk if i == 1 else pd.concat([performance_data, chunk])\n",
    "        i += 1\n",
    "\n",
    "    performance_data['year'] = int(year)\n",
    "    \n",
    "    print('Extracting', year, 'performance data: done')\n",
    "    \n",
    "    performance_data = edit_data(performance_data, cols)\n",
    "    \n",
    "    return performance_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting 2019 performance data: done\n",
      "Editing...\n",
      "mba_delinquency_status\n",
      "current_investor_code\n",
      "current_product_type\n",
      "mba_worst_ever\n",
      "active_status\n",
      "bk_flag\n",
      "Done editing. Time Lapsed = 0:25:47 \n",
      "\n",
      "Extracting 2018 performance data: done\n",
      "Editing...\n",
      "mba_delinquency_status\n",
      "current_investor_code\n",
      "current_product_type\n",
      "mba_worst_ever\n",
      "active_status\n",
      "bk_flag\n",
      "Done editing. Time Lapsed = 0:55:51 \n",
      "\n",
      "\n",
      "Appending 18-19 performance data: done \n",
      "\n",
      "Time Lapsed = 0:57:38 \n",
      "\n",
      "Extracting 2017 performance data: done\n",
      "Editing...\n",
      "mba_delinquency_status\n",
      "current_investor_code\n",
      "current_product_type\n",
      "mba_worst_ever\n",
      "active_status\n",
      "bk_flag\n",
      "Done editing. Time Lapsed = 1:30:24 \n",
      "\n",
      "\n",
      "Appending 17 performance data: done \n",
      "\n",
      "Time Lapsed = 1:34:11 \n",
      "\n",
      "Extracting 2016 performance data: done\n",
      "Editing...\n",
      "mba_delinquency_status\n",
      "current_investor_code\n",
      "current_product_type\n",
      "mba_worst_ever\n",
      "active_status\n",
      "bk_flag\n",
      "Done editing. Time Lapsed = 2:8:24 \n",
      "\n",
      "\n",
      "Appending 16 performance data: done \n",
      "\n",
      "Time Lapsed = 2:14:19 \n",
      "\n",
      "Extracting 2015 performance data: done\n",
      "Editing...\n",
      "mba_delinquency_status\n",
      "current_investor_code\n",
      "current_product_type\n",
      "mba_worst_ever\n",
      "active_status\n",
      "bk_flag\n",
      "Done editing. Time Lapsed = 2:53:27 \n",
      "\n",
      "\n",
      "Appending 15 performance data: done \n",
      "\n",
      "Time Lapsed = 3:1:54 \n",
      "\n",
      "Extracting 2014 performance data: done\n",
      "Editing...\n",
      "mba_delinquency_status\n",
      "current_investor_code\n",
      "current_product_type\n",
      "mba_worst_ever\n",
      "active_status\n",
      "bk_flag\n",
      "Done editing. Time Lapsed = 3:43:6 \n",
      "\n",
      "\n",
      "Appending 14 performance data: done \n",
      "\n",
      "Time Lapsed = 3:53:13 \n",
      "\n",
      "Extracting 2013 performance data: done\n",
      "Editing...\n",
      "mba_delinquency_status\n",
      "current_investor_code\n",
      "current_product_type\n",
      "mba_worst_ever\n",
      "active_status\n",
      "bk_flag\n",
      "Done editing. Time Lapsed = 4:34:29 \n",
      "\n",
      "\n",
      "Appending 13 performance data: done \n",
      "\n",
      "Time Lapsed = 4:47:38 \n",
      "\n",
      "Extracting 2012 performance data: done\n",
      "Editing...\n",
      "mba_delinquency_status\n",
      "current_investor_code\n",
      "current_product_type\n",
      "mba_worst_ever\n",
      "active_status\n",
      "bk_flag\n",
      "Done editing. Time Lapsed = 5:29:12 \n",
      "\n",
      "\n",
      "Appending 12 performance data: done \n",
      "\n",
      "Time Lapsed = 5:47:3 \n",
      "\n",
      "Extracting 2011 performance data: done\n",
      "Editing...\n",
      "mba_delinquency_status\n",
      "current_investor_code\n",
      "current_product_type\n",
      "mba_worst_ever\n",
      "active_status\n",
      "bk_flag\n",
      "Done editing. Time Lapsed = 6:28:45 \n",
      "\n",
      "\n",
      "Appending 11 performance data: done \n",
      "\n",
      "Time Lapsed = 6:49:33 \n",
      "\n",
      "Extracting 2010 performance data: done\n",
      "Editing...\n",
      "mba_delinquency_status\n",
      "current_investor_code\n",
      "current_product_type\n",
      "mba_worst_ever\n",
      "active_status\n",
      "bk_flag\n",
      "Done editing. Time Lapsed = 7:23:13 \n",
      "\n",
      "\n",
      "Appending 10 performance data: done \n",
      "\n",
      "Time Lapsed = 7:46:18 \n",
      "\n",
      "Extracting 2009 performance data: done\n",
      "Editing...\n",
      "mba_delinquency_status\n",
      "current_investor_code\n",
      "current_product_type\n",
      "mba_worst_ever\n",
      "active_status\n",
      "bk_flag\n",
      "Done editing. Time Lapsed = 8:16:45 \n",
      "\n",
      "\n",
      "Appending 09 performance data: done \n",
      "\n",
      "Time Lapsed = 8:44:51 \n",
      "\n",
      "Extracting 2008 performance data: done\n",
      "Editing...\n",
      "mba_delinquency_status\n",
      "current_investor_code\n",
      "current_product_type\n",
      "mba_worst_ever\n",
      "active_status\n",
      "bk_flag\n",
      "Done editing. Time Lapsed = 9:17:8 \n",
      "\n",
      "\n",
      "Appending 08 performance data: done \n",
      "\n",
      "Time Lapsed = 9:47:50 \n",
      "\n",
      "\n",
      "Appending performance data: done\n"
     ]
    }
   ],
   "source": [
    "performance_2019 = extract_performance_data('2019')\n",
    "performance_2018 = extract_performance_data('2018')\n",
    "all_performances = performance_2019.append(performance_2018)\n",
    "del performance_2019\n",
    "del performance_2018\n",
    "gc.collect()\n",
    "\n",
    "print('\\nAppending 18-19 performance data: done \\n')\n",
    "print(time_convert(time.time() - start_time), '\\n')\n",
    "\n",
    "performance_2017 = extract_performance_data('2017')\n",
    "all_performances = all_performances.append(performance_2017)\n",
    "del performance_2017\n",
    "gc.collect()\n",
    "\n",
    "print('\\nAppending 17 performance data: done \\n')\n",
    "print(time_convert(time.time() - start_time), '\\n')\n",
    "\n",
    "performance_2016 = extract_performance_data('2016')\n",
    "all_performances = all_performances.append(performance_2016)\n",
    "del performance_2016\n",
    "gc.collect()\n",
    "\n",
    "print('\\nAppending 16 performance data: done \\n')\n",
    "print(time_convert(time.time() - start_time), '\\n')\n",
    "\n",
    "performance_2015 = extract_performance_data('2015')\n",
    "all_performances = all_performances.append(performance_2015)\n",
    "del performance_2015\n",
    "gc.collect()\n",
    "\n",
    "print('\\nAppending 15 performance data: done \\n')\n",
    "print(time_convert(time.time() - start_time), '\\n')\n",
    "\n",
    "performance_2014 = extract_performance_data('2014')\n",
    "all_performances = all_performances.append(performance_2014)\n",
    "del performance_2014\n",
    "gc.collect()\n",
    "\n",
    "print('\\nAppending 14 performance data: done \\n')\n",
    "print(time_convert(time.time() - start_time), '\\n')\n",
    "\n",
    "performance_2013 = extract_performance_data('2013')\n",
    "all_performances = all_performances.append(performance_2013)\n",
    "del performance_2013\n",
    "gc.collect()\n",
    "\n",
    "print('\\nAppending 13 performance data: done \\n')\n",
    "print(time_convert(time.time() - start_time), '\\n')\n",
    "\n",
    "performance_2012 = extract_performance_data('2012')\n",
    "all_performances = all_performances.append(performance_2012)\n",
    "del performance_2012\n",
    "gc.collect()\n",
    "\n",
    "print('\\nAppending 12 performance data: done \\n')\n",
    "print(time_convert(time.time() - start_time), '\\n')\n",
    "\n",
    "performance_2011 = extract_performance_data('2011')\n",
    "all_performances = all_performances.append(performance_2011)\n",
    "del performance_2011\n",
    "gc.collect()\n",
    "\n",
    "print('\\nAppending 11 performance data: done \\n')\n",
    "print(time_convert(time.time() - start_time), '\\n')\n",
    "\n",
    "performance_2010 = extract_performance_data('2010')\n",
    "all_performances = all_performances.append(performance_2010)\n",
    "del performance_2010\n",
    "gc.collect()\n",
    "\n",
    "print('\\nAppending 10 performance data: done \\n')\n",
    "print(time_convert(time.time() - start_time), '\\n')\n",
    "\n",
    "performance_2009 = extract_performance_data('2009')\n",
    "all_performances = all_performances.append(performance_2009)\n",
    "del performance_2009\n",
    "gc.collect()\n",
    "\n",
    "print('\\nAppending 09 performance data: done \\n')\n",
    "print(time_convert(time.time() - start_time), '\\n')\n",
    "\n",
    "performance_2008 = extract_performance_data('2008')\n",
    "all_performances = all_performances.append(performance_2008)\n",
    "del performance_2008\n",
    "gc.collect()\n",
    "\n",
    "print('\\nAppending 08 performance data: done \\n')\n",
    "print(time_convert(time.time() - start_time), '\\n')\n",
    "\n",
    "print('\\nAppending performance data: done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Lapsed = 25:7:23\n"
     ]
    }
   ],
   "source": [
    "all_performances.to_csv('PERFORMANCES.csv')\n",
    "print(time_convert(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Origination Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_origination_data(filename):\n",
    "    \n",
    "    print('Extracting {}'.format(filename))\n",
    "    \n",
    "    cols = ['loan_id', 'origination_date', 'property_zip', 'state', 'property_type', \n",
    "            'number_of_units', 'occupancy_type', 'original_balance', \n",
    "            'sale_price', 'appraised_value', 'product_type', 'original_term',\n",
    "            'initial_interest_rate', 'back_end_ratio', 'loan_type', \n",
    "            'loan_purpose', 'payment_frequency', 'channel', 'buydown_flag', \n",
    "            'documentation_type',  'convertible_flag', 'pool_insurance_flag', 'original_ltv', \n",
    "            'negative_amortization_flag', 'margin', 'periodic_rate_cap', \n",
    "            'periodic_rate_floor', 'lifetime_rate_cap', 'lifetime_rate_floor',\n",
    "            'rate_reset_frequency', 'pay_reset_frequency', 'first_rate_reset_period', \n",
    "            'fico_score_at_origination', 'prepay_penalty_flag', 'prepay_penalty_term', \n",
    "            'combined_ltv_at_origination', 'cbsa', 'io_term', 'io_flag', \n",
    "            'msa', 'paid_off_flag', 'inferred_collateral_type', 'collateral_type', \n",
    "            'orig_active_status', 'period', 'product_type_category', 'loan_purpose_category', \n",
    "            'mortgage_insurance_flag', 'gse_eligible_flag', 'payment_frequency']\n",
    "    \n",
    "    origination_data = pd.read_csv(filename, sep='|', low_memory=False, usecols = cols)\n",
    "        \n",
    "    origination_data = edit_data(origination_data, cols)\n",
    "        \n",
    "    return origination_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Inactive_Origination_Firsts_2008_2010.txt\n",
      "Editing...\n",
      "property_type\n",
      "occupancy_type\n",
      "product_type\n",
      "loan_type\n",
      "loan_purpose\n",
      "payment_frequency\n",
      "channel\n",
      "documentation_type\n",
      "orig_active_status\n",
      "gse_eligible_flag\n",
      "payment_frequency\n",
      "buydown_flag\n",
      "convertible_flag\n",
      "pool_insurance_flag\n",
      "negative_amortization_flag\n",
      "prepay_penalty_flag\n",
      "io_flag\n",
      "paid_off_flag\n",
      "inferred_collateral_type\n",
      "collateral_type\n",
      "product_type_category\n",
      "loan_purpose_category\n",
      "mortgage_insurance_flag\n",
      "Done editing. Time Lapsed = 25:9:5 \n",
      "\n",
      "Extracting Recent_Origination_Firsts_202007.txt\n",
      "Editing...\n",
      "property_type\n",
      "occupancy_type\n",
      "product_type\n",
      "loan_type\n",
      "loan_purpose\n",
      "payment_frequency\n",
      "channel\n",
      "documentation_type\n",
      "orig_active_status\n",
      "gse_eligible_flag\n",
      "payment_frequency\n",
      "buydown_flag\n",
      "convertible_flag\n",
      "pool_insurance_flag\n",
      "negative_amortization_flag\n",
      "prepay_penalty_flag\n",
      "io_flag\n",
      "paid_off_flag\n",
      "inferred_collateral_type\n",
      "collateral_type\n",
      "product_type_category\n",
      "loan_purpose_category\n",
      "mortgage_insurance_flag\n",
      "Done editing. Time Lapsed = 25:21:47 \n",
      "\n",
      "\n",
      "Appending origination data: done\n",
      "Time Lapsed = 25:22:34\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inactive_origination_2008_2010 = extract_origination_data('Inactive_Origination_Firsts_2008_2010.txt')\n",
    "\n",
    "active_origination_202007 = extract_origination_data('Recent_Origination_Firsts_202007.txt')\n",
    "\n",
    "all_origination = inactive_origination_2008_2010.append(active_origination_202007)\n",
    "print('\\nAppending origination data: done')\n",
    "print(time_convert(time.time() - start_time))\n",
    "\n",
    "del inactive_origination_2008_2010\n",
    "del active_origination_202007\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Lapsed = 25:48:24\n"
     ]
    }
   ],
   "source": [
    "all_origination.to_csv('ORIGINATION.csv')\n",
    "print(time_convert(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save as Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neg_cols = get_ranges(performances)\n",
    "# print(neg_cols)\n",
    "# # performances = remove_strange(performances, neg_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Performances Dataset***\n",
      "Shape before: (4995041139, 16)\n",
      "Shape after: (4995041139, 16) \n",
      "\n",
      "***Origination Dataset***\n",
      "Shape before: (72399685, 49)\n",
      "Shape after: (72399685, 38) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_performances = drop_columns(all_performances, 0.75)\n",
    "all_origination = drop_columns(all_origination, 0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=1\n",
    "for chunk in pd.read_csv('PERFORMANCES.csv', low_memory=False, chunksize=20000000):\n",
    "    all_performances = chunk if i == 1 else pd.concat([all_performances, chunk])\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_origination = pd.read_csv('ORIGINATION.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = all_performances.merge(all_origination, on='loan_id', how='inner', validate='many_to_one')\n",
    "print(time_convert(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv('MERGED_orig2.csv')\n",
    "print(time_convert(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['origination_date'] = merged_df['origination_date'].astype(int)\n",
    "unique_dates = merged_df['origination_date'].unique()\n",
    "unique_dates = unique_dates[unique_dates > 200800]\n",
    "\n",
    "for date in unique_dates:\n",
    "    for state in merged_df['state'].unique():\n",
    "        file_name = 'MERGED_{}_{}.csv'.format(date, state)\n",
    "        condition = (merged_df['origination_date'] == date) & (merged_df['state'] == state)\n",
    "        merged_df[condition].to_csv(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('DONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
