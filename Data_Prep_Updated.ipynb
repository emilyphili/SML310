{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for Extracting CoreLogic Data\n",
    "Emily Philippides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc \n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "print(platform.architecture())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd E:\\Thesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('STARTING')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code execution timer\n",
    "# Code taken from: https://www.codespeedy.com/how-to-create-a-stopwatch-in-python/\n",
    "def time_convert(sec):\n",
    "    mins = sec // 60\n",
    "    sec = sec % 60\n",
    "    hours = mins // 60\n",
    "    mins = mins % 60\n",
    "    return (\"Time Lapsed = {}:{}:{}\".format(int(hours), int(mins), int(sec)))\n",
    "\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Properly encode 'missing' values as NaN\n",
    "def edit_data(df, cols):\n",
    "    print('Editing...')\n",
    "    \n",
    "    # Convert categorical variables 'missing' values to NaN\n",
    "    cat = ['mba_delinquency_status', #  X = Unknown/Missing LPI date\n",
    "    'orig_active_status',\n",
    "    'current_investor_code', # U = No Info\n",
    "    'current_product_type', # NULL = No Info\n",
    "    'mba_worst_ever', # U = No Info\n",
    "    'property_type', # U = No Info\n",
    "    'occupancy_type', # U = No Info\n",
    "    'product_type', # U = No Info\n",
    "    'loan_type', # U = No Info\n",
    "    'loan_purpose', # U = No Info\n",
    "    'payment_frequency', # U = No Info\n",
    "    'channel', # U = No Info\n",
    "    'documentation_type',  # U = No Info\n",
    "    'gse_eligible_flag',  # U = No Info\n",
    "    'payment_frequency'] # U = No Info\n",
    "    \n",
    "    c_cat = []\n",
    "    for i in cols:\n",
    "        if i in cat:\n",
    "            c_cat.append(i)\n",
    "    \n",
    "    for i in c_cat:\n",
    "        print('-{}'.format(i))\n",
    "        if True in df[i].astype(str).isin(['U']).values.tolist():\n",
    "            df.loc[(df[i] == 'U'), i] = np.NaN\n",
    "        elif True in df[i].astype(str).isin(['X']).values.tolist():\n",
    "            df.loc[(df[i] == 'X'), i] = np.NaN\n",
    "        elif True in df[i].astype(str).isin(['NULL']).values.tolist():\n",
    "            df.loc[(df[i] == 'NULL'), i] = np.NaN\n",
    "            \n",
    "    # Convert binary variables 'missing' values to NaN\n",
    "    dummies = ['prepay_penalty_flag',\n",
    "    'collateral_type', \n",
    "    'product_type_category', \n",
    "    'loan_purpose_category', \n",
    "    'mortgage_insurance_flag', \n",
    "    'active_status', # no NA\n",
    "    'bk_flag', # no NA\n",
    "    'buydown_flag', \n",
    "    'convertible_flag', # no NA\n",
    "    'pool_insurance_flag', \n",
    "    'negative_amortization_flag', \n",
    "    'io_flag', \n",
    "    'paid_off_flag', \n",
    "    'inferred_collateral_type']\n",
    "\n",
    "    c_dum = []\n",
    "    for i in cols:\n",
    "        if i in dummies:\n",
    "            c_dum.append(i)\n",
    "    \n",
    "    for i in c_dum:\n",
    "        print('-{}'.format(i))\n",
    "        unique_values = df[i].astype(str).unique().tolist()\n",
    "        if True in df[i].astype(str).isin(['U']).values.tolist():\n",
    "            unique_values.remove('U')\n",
    "            df.loc[(df[i] == 'U'), i] = np.NaN\n",
    "        \n",
    "        # Ensure binary variables are indeed binary\n",
    "        if len(unique_values) > 2:\n",
    "            print(\"ERROR!!!!!! More than 2 unique values\")\n",
    "            print(unique_values, '| Removing...', unique_values[2])\n",
    "            df.loc[(df[i] == unique_values[2]).values, i] = np.NaN        \n",
    "    \n",
    "    print('Done editing. {} \\n'.format(time_convert(time.time() - start_time)))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of performance instances for each unique loan\n",
    "def add_loan_counts(df):\n",
    "    df['one'] = 1\n",
    "    counts_df = df.groupby(by=['loan_id'])[['one']].sum().reset_index()\n",
    "    counts_df.columns = ['loan_id', 'counts']\n",
    "    df = df.merge(counts_df, on='loan_id', how='inner', validate='many_to_one')\n",
    "    df = df.drop(columns=['one'])\n",
    "    print('Done merging \\n')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get range of values of each column in specified dataframe\n",
    "def get_ranges(df):\n",
    "    neg_numbers = []\n",
    "    df = df.select_dtypes(include=np.number)\n",
    "    for col in df.columns:\n",
    "        print('{} range of values: [{}, {}]'.format(col, df[col].min(), df[col].max()))\n",
    "        if df[col].min() < 0:\n",
    "            neg_numbers.append(col)\n",
    "    return neg_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with strange (e.g. negative) values in specified columns\n",
    "def clean_rows(df, neg_cols):\n",
    "    for col in neg_cols:\n",
    "        print(col)\n",
    "        display(df[df[col] < 0])\n",
    "        df = df[df[col] > 0]\n",
    "\n",
    "    display(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of null values in each column and each row\n",
    "def count_null(df, thresh):\n",
    "    print('***{}***'.format(get_df_name(df)))\n",
    "    print('Columns = {}. Rows = {}.'.format(df.shape[1], df.shape[0]))\n",
    "    t = str(int(thresh*100))\n",
    "\n",
    "    null_rows = df.isnull().sum(axis=1).values\n",
    "    fiftyp_rows = null_rows/len(df.columns)\n",
    "    fiftyp_rows = len(fiftyp_rows[fiftyp_rows>thresh])\n",
    "    print('There are no rows with more than {} NaN columns. {} rows have more than {}% NaN columns.'.format(null_rows.max(), fiftyp_rows, t))\n",
    "\n",
    "    null_cols = df.isnull().sum(axis=0).values\n",
    "    fiftyp_cols = null_cols/len(df)\n",
    "    fiftyp_cols = len(fiftyp_cols[fiftyp_cols>thresh])\n",
    "    print('There are no columns with more than {} NaN rows. {} columns have more than {}% NaN rows. \\n'.format(null_cols.max(), fiftyp_cols, t))\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove columns with > x NaN values in the rows\n",
    "def drop_columns(df, thresh):\n",
    "    print('***{}***'.format(get_df_name(df)))\n",
    "    print('Shape before: {}'.format(df.shape))\n",
    "    cutoff = len(df) * thresh\n",
    "    df = df.drop(columns = (df.columns[df.isna().sum().values > cutoff]))\n",
    "    print('Shape after: {} \\n'.format(df.shape))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Origination Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_origination_data(filename):\n",
    "    \n",
    "    print('Extracting {}'.format(filename))\n",
    "    \n",
    "    cols = ['loan_id', 'origination_date', 'property_zip', 'state', 'property_type', \n",
    "            'number_of_units', 'occupancy_type', 'original_balance', \n",
    "            'sale_price', 'appraised_value', 'product_type', 'original_term',\n",
    "            'initial_interest_rate', 'back_end_ratio', 'loan_type', \n",
    "            'loan_purpose', 'payment_frequency', 'channel', 'buydown_flag', \n",
    "            'documentation_type',  'convertible_flag', 'pool_insurance_flag', 'original_ltv', \n",
    "            'negative_amortization_flag', 'margin', 'periodic_rate_cap', \n",
    "            'periodic_rate_floor', 'lifetime_rate_cap', 'lifetime_rate_floor',\n",
    "            'rate_reset_frequency', 'pay_reset_frequency', 'first_rate_reset_period', \n",
    "            'fico_score_at_origination', 'prepay_penalty_flag', 'prepay_penalty_term', \n",
    "            'combined_ltv_at_origination', 'cbsa', 'io_term', 'io_flag', \n",
    "            'msa', 'paid_off_flag', 'inferred_collateral_type', 'collateral_type', \n",
    "            'orig_active_status', 'period', 'product_type_category', 'loan_purpose_category', \n",
    "            'mortgage_insurance_flag', 'gse_eligible_flag', 'payment_frequency']\n",
    "    \n",
    "    origination_data = pd.read_csv(filename, sep='|', low_memory=False, usecols = cols)\n",
    "    \n",
    "    print('Done extracting. {}'.format(time_convert(time.time() - start_time)))\n",
    "        \n",
    "    origination_data = edit_data(origination_data, cols)\n",
    "        \n",
    "    return origination_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load origination records for all mortgages originated 2008-2010\n",
    "inactive_origination_2008_2010 = extract_origination_data('Inactive_Origination_Firsts_2008_2010.txt')\n",
    "\n",
    "# Load origination records for all mortgages originated December 2013 - July 2020\n",
    "active_origination_202007 = extract_origination_data('Recent_Origination_Firsts_202007.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge dataframes\n",
    "all_origination = inactive_origination_2008_2010.append(active_origination_202007)\n",
    "print('\\nDone appending origination data. {}'.format(time_convert(time.time() - start_time)))\n",
    "del inactive_origination_2008_2010\n",
    "del active_origination_202007\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataframe\n",
    "# all_origination.to_csv('ORIGINATION.csv')\n",
    "# print(time_convert(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Data\n",
    "Load performance data and merge with origination data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_performance_data(year, all_origination):\n",
    "    \n",
    "    print('{} performance data'.format(year))\n",
    "    \n",
    "    cols = ['loan_id', 'current_balance', 'current_interest_rate', \n",
    "             'total_payment_due', 'scheduled_principal', 'scheduled_monthly_pi', \n",
    "             'mba_delinquency_status', 'mba_days_delinquent', 'active_status', \n",
    "             'period_of_payment', 'current_investor_code', 'current_product_type', \n",
    "             'loan_age', 'mba_worst_ever', 'bk_flag']\n",
    "    \n",
    "    i=1\n",
    "    name = 'Performance_Firsts_' + year\n",
    "    for chunk in pd.read_csv((name + '/' + name + '.txt'), sep='|', usecols = cols, low_memory=False, chunksize=20000000):\n",
    "        performance_data = chunk if i == 1 else pd.concat([performance_data, chunk])\n",
    "        i += 1\n",
    "\n",
    "    performance_data['year'] = int(year)\n",
    "    \n",
    "    print('Done extracting. {}'.format(time_convert(time.time() - start_time)))\n",
    "    \n",
    "    merged_df = performance_data.merge(all_origination, on='loan_id', how='inner', validate='many_to_one')\n",
    "    \n",
    "    print('Done merging. {}'.format(time_convert(time.time() - start_time)))\n",
    "    \n",
    "    del performance_data\n",
    "    gc.collect()\n",
    "    \n",
    "    merged_df = edit_data(merged_df, cols)\n",
    "    \n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load performance data for all active mortgages between 2008 and 2019\n",
    "data_2019 = extract_performance_data('2019', all_origination)\n",
    "data_2018 = extract_performance_data('2018', all_origination)\n",
    "data_2017 = extract_performance_data('2017', all_origination)\n",
    "data_2016 = extract_performance_data('2016', all_origination)\n",
    "data_2015 = extract_performance_data('2015', all_origination)\n",
    "data_2014 = extract_performance_data('2014', all_origination)\n",
    "data_2013 = extract_performance_data('2013', all_origination)\n",
    "data_2012 = extract_performance_data('2012', all_origination)\n",
    "data_2011 = extract_performance_data('2011', all_origination)\n",
    "data_2010 = extract_performance_data('2010', all_origination)\n",
    "data_2009 = extract_performance_data('2009', all_origination)\n",
    "data_2008 = extract_performance_data('2008', all_origination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append all yearly data into one dataframe\n",
    "merged_df = data_2019.append(data_2018)\n",
    "print('Done appending 18-19 data. {} \\n'.format(time_convert(time.time() - start_time)))\n",
    "del data_2019\n",
    "del data_2018\n",
    "gc.collect()\n",
    "\n",
    "merged_df = merged_df.append(data_2017)\n",
    "print('Done appending 17 data. {} \\n'.format(time_convert(time.time() - start_time)))\n",
    "del data_2017\n",
    "gc.collect()\n",
    "\n",
    "merged_df = merged_df.append(data_2016)\n",
    "print('Done appending 16 data. {} \\n'.format(time_convert(time.time() - start_time)))\n",
    "del data_2016\n",
    "gc.collect()\n",
    "\n",
    "merged_df = merged_df.append(data_2015)\n",
    "print('Done appending 15 data. {} \\n'.format(time_convert(time.time() - start_time)))\n",
    "del data_2015\n",
    "gc.collect()\n",
    "\n",
    "merged_df = merged_df.append(data_2014)\n",
    "print('Done appending 14 data. {} \\n'.format(time_convert(time.time() - start_time)))\n",
    "del data_2014\n",
    "gc.collect()\n",
    "\n",
    "merged_df = merged_df.append(data_2013)\n",
    "print('Done appending 13 data. {} \\n'.format(time_convert(time.time() - start_time)))\n",
    "del data_2013\n",
    "gc.collect()\n",
    "\n",
    "merged_df = merged_df.append(data_2012)\n",
    "print('Done appending 12 data. {} \\n'.format(time_convert(time.time() - start_time)))\n",
    "del data_2012\n",
    "gc.collect()\n",
    "\n",
    "merged_df = merged_df.append(data_2011)\n",
    "print('Done appending 11 data. {} \\n'.format(time_convert(time.time() - start_time)))\n",
    "del data_2011\n",
    "gc.collect()\n",
    "\n",
    "merged_df = merged_df.append(data_2010)\n",
    "print('Done appending 10 data. {} \\n'.format(time_convert(time.time() - start_time)))\n",
    "del data_2010\n",
    "gc.collect()\n",
    "\n",
    "merged_df = merged_df.append(data_2009)\n",
    "print('Done appending 09 data. {} \\n'.format(time_convert(time.time() - start_time)))\n",
    "del data_2009\n",
    "gc.collect()\n",
    "\n",
    "merged_df = merged_df.append(data_2008)\n",
    "print('Done appending 08 data. {} \\n'.format(time_convert(time.time() - start_time)))\n",
    "del data_2008\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove columns with >= 75% NaN values in the rows\n",
    "merged_df = drop_columns(merged_df, 0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataframe\n",
    "# merged_df.to_csv('MERGED.csv')\n",
    "# print(time_convert(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Smaller Chunks\n",
    "By state and origination date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd F:\\Thesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all unique origination dates (formatted as YYYYMM)\n",
    "merged_df['origination_date'] = merged_df['origination_date'].astype(int)\n",
    "unique_dates = merged_df['origination_date'].unique()\n",
    "unique_dates = unique_dates[unique_dates > 200800]\n",
    "print(unique_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split and save dataframe by state, origination date\n",
    "for date in unique_dates:\n",
    "    for state in merged_df['state'].unique():\n",
    "        file_name = 'MERGED_{}_{}.csv'.format(state, date)\n",
    "        condition = (merged_df['origination_date'] == date) & (merged_df['state'] == state)\n",
    "        merged_df[condition].to_csv(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('DONE')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
